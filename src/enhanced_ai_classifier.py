#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆAIåˆ†ç±»å™¨ - æ”¯æŒå¤šç§æ¨¡å‹
å…¼å®¹åŸæœ‰åŠŸèƒ½ + æ–°å¢DeepDanbooruäºŒæ¬¡å…ƒè¯†åˆ«
"""
import sys
import os
import json
import time
import numpy as np
from PIL import Image

# æŠ‘åˆ¶TensorFlowæ—¥å¿—è¾“å‡º
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # åªæ˜¾ç¤ºERRORå’ŒFATAL
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # å…³é—­oneDNNä¼˜åŒ–ä¿¡æ¯

import tensorflow as tf
from pathlib import Path

# è¿›ä¸€æ­¥æŠ‘åˆ¶TensorFlowæ—¥å¿—
tf.get_logger().setLevel('ERROR')

class EnhancedAIClassifier:
    def __init__(self):
        self.app_dir = Path(sys.argv[0]).parent if hasattr(sys, 'argv') and len(sys.argv) > 0 else Path.cwd()
        self.models_dir = self.app_dir / "models"
        print(f"åº”ç”¨ç›®å½•: {self.app_dir}")
        print(f"æ¨¡å‹ç›®å½•: {self.models_dir}")

    def classify_original(self, image_path):
        """åŸæœ‰çš„åˆ†ç±»åŠŸèƒ½ - ä½¿ç”¨ImageNeté¢„è®­ç»ƒæ¨¡å‹"""
        try:
            print("ğŸ¤– å¼€å§‹é€šç”¨ç‰©ä½“è¯†åˆ«...")

            # åŠ è½½ImageNeté¢„è®­ç»ƒæ¨¡å‹
            print("ğŸ”„ åŠ è½½MobileNetV2æ¨¡å‹...")
            model = tf.keras.applications.MobileNetV2(
                weights='imagenet',
                include_top=True,
                input_shape=(224, 224, 3)
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # é¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # ImageNetä½¿ç”¨224x224
            image = image.resize((224, 224), Image.Resampling.LANCZOS)
            image_array = np.array(image, dtype=np.float32)

            # MobileNetV2é¢„å¤„ç†
            image_array = tf.keras.applications.mobilenet_v2.preprocess_input(image_array)
            image_array = np.expand_dims(image_array, axis=0)

            print("ğŸ¤– å¼€å§‹AIæ¨ç†...")
            # æ¨¡å‹æ¨ç†
            start_time = time.time()
            predictions = model.predict(image_array, verbose=0)
            processing_time = time.time() - start_time

            # è§£ç é¢„æµ‹ç»“æœ
            decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(
                predictions, top=5
            )[0]

            # ç®€æ´è¾“å‡º
            labels = [label.replace('_', ' ') for _, label, _ in decoded_predictions]
            return "é€šç”¨è¯†åˆ«: " + ", ".join(labels)

        except Exception as e:
            return f"é€šç”¨è¯†åˆ«é”™è¯¯: {str(e)}"

    def classify_anime(self, image_path):
        """äºŒæ¬¡å…ƒé£æ ¼è¯†åˆ«"""
        try:
            print("ğŸ¨ å¼€å§‹äºŒæ¬¡å…ƒé£æ ¼è¯†åˆ«...")

            # æ£€æŸ¥DeepDanbooruæ¨¡å‹æ–‡ä»¶
            deepdanbooru_dir = self.models_dir / "deepdanbooru"
            model_path = deepdanbooru_dir / "model-resnet_custom_v3.h5"
            tags_path = deepdanbooru_dir / "tags.txt"

            if not model_path.exists():
                return f"âŒ DeepDanbooruæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"

            if not tags_path.exists():
                return f"âŒ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {tags_path}"

            print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
            print(f"âœ… æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {tags_path}")

            # åŠ è½½æ ‡ç­¾
            with open(tags_path, 'r', encoding='utf-8') as f:
                tags = [line.strip() for line in f.readlines()]
            print(f"âœ… åŠ è½½äº† {len(tags)} ä¸ªæ ‡ç­¾")

            # åŠ è½½æ¨¡å‹
            print("ğŸ”„ åŠ è½½DeepDanbooruæ¨¡å‹...")
            model = tf.keras.models.load_model(str(model_path), compile=False)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # é¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # DeepDanboorué€šå¸¸ä½¿ç”¨512x512
            image = image.resize((512, 512), Image.Resampling.LANCZOS)
            image_array = np.array(image, dtype=np.float32)
            image_array = image_array / 255.0  # å½’ä¸€åŒ–
            image_array = np.expand_dims(image_array, axis=0)

            print("ğŸ¤– å¼€å§‹AIæ¨ç†...")
            # æ¨¡å‹æ¨ç†
            start_time = time.time()
            predictions = model.predict(image_array, verbose=0)
            processing_time = time.time() - start_time

            # è§£æç»“æœ
            scores = predictions[0] if len(predictions.shape) > 1 else predictions
            threshold = 0.3  # ç½®ä¿¡åº¦é˜ˆå€¼

            results = []
            for i, score in enumerate(scores):
                if i < len(tags) and score >= threshold:
                    results.append({
                        "tag": tags[i],
                        "confidence": float(score)
                    })

            # æŒ‰ç½®ä¿¡åº¦æ’åº
            results.sort(key=lambda x: x["confidence"], reverse=True)
            results = results[:15]  # å–å‰15ä¸ª

            # ç®€æ´è¾“å‡º
            if results:
                tag_names = [result["tag"] for result in results]
                return "äºŒæ¬¡å…ƒæ ‡ç­¾: " + ", ".join(tag_names)
            else:
                return "æœªè¯†åˆ«åˆ°ä»»ä½•äºŒæ¬¡å…ƒæ ‡ç­¾"

        except Exception as e:
            return f"âŒ äºŒæ¬¡å…ƒè¯†åˆ«å¤±è´¥: {str(e)}"

def main():
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python enhanced_ai_classifier.py <å›¾ç‰‡è·¯å¾„> <æ¨¡å¼>")
        print("æ¨¡å¼: original | anime")
        sys.exit(1)

    image_path = sys.argv[1]
    mode = sys.argv[2]

    # å¤„ç†file://åè®®çš„è·¯å¾„
    if image_path.startswith("file:///"):
        image_path = image_path[8:]  # ç§»é™¤file:///
        import urllib.parse
        image_path = urllib.parse.unquote(image_path)
    elif image_path.startswith("file://"):
        image_path = image_path[7:]  # ç§»é™¤file://
        import urllib.parse
        image_path = urllib.parse.unquote(image_path)

    print(f"å¤„ç†åçš„å›¾ç‰‡è·¯å¾„: {image_path}")

    if not os.path.exists(image_path):
        print(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
        sys.exit(1)

    classifier = EnhancedAIClassifier()

    if mode == "anime":
        result = classifier.classify_anime(image_path)
    elif mode == "original":
        result = classifier.classify_original(image_path)
    else:
        result = f"æœªçŸ¥æ¨¡å¼: {mode}ï¼Œæ”¯æŒçš„æ¨¡å¼: original, anime"

    print(result)

if __name__ == "__main__":
    main()
